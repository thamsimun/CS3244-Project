{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS3244 Machine Learning Team Project\n",
    "## Prediction of the number of dengue cases in Singapore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we shall import necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import data processing tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import Levenshtein as lev\n",
    "\n",
    "import csv, json, sys\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Import visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import utility tools\n",
    "import requests\n",
    "from math import *\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# Import ML tools\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall read from various CSV files to extract dataframe for each feature in the following cells:\n",
    "* Weekly **dengue** cases (*dengue_df*)\n",
    "* Annual **population** (*population_df*)\n",
    "* Monthly mean **sunshine** hours (*sunshine_df*)\n",
    "* Monthly **surface air temperature** (*surface_air_temperature_df*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read weekly dengue cases from csv files\n",
    "raw_dengue_df = pd.read_csv(\n",
    "    \"Datasets/Dengue Cluster Data/weekly-number-of-dengue-and-dengue-haemorrhagic-fever-cases.csv\")\n",
    "\n",
    "# Clean data by filtering by type_dengue and remove the redundant column\n",
    "dengue_df = raw_dengue_df[raw_dengue_df['type_dengue'].map(\n",
    "    lambda x: str(x) == \"Dengue\")].drop('type_dengue', 1).reset_index()\n",
    "\n",
    "# Rename eweek to week\n",
    "dengue_df = dengue_df.rename(columns={\"eweek\": \"week\"}, errors=\"raise\")\n",
    "# Display first 5 rows of the dataframe\n",
    "dengue_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from CSV\n",
    "raw_population_df = pd.read_csv(\n",
    "    \"Datasets/Population Data/singapore-residents-by-age-group-ethnic-group-and-sex-end-june-annual.csv\")\n",
    "# Only get data from 2014 to 2018\n",
    "population_df = raw_population_df[raw_population_df['year'].map(\n",
    "    lambda x: 2014 <= int(x) <= 2018)]\n",
    "# Only get total residents for each year\n",
    "population_df = population_df[population_df['level_1'].map(\n",
    "    lambda x: str(x) == 'Total Residents')].drop('level_1', 1).drop('level_2', 1)\n",
    "population_df = population_df.astype({'year' : 'int32', 'value': 'int32'})\n",
    "population_df = population_df.groupby('year').sum()\n",
    "population_df = population_df.rename(columns={\"value\": \"population\"}, errors=\"raise\")\n",
    "# Display dataframe\n",
    "population_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from CSV\n",
    "raw_sunshine_df = pd.read_csv(\"Datasets/Sunshine Data/sunshine-duration-monthly-mean-daily-duration.csv\")\n",
    "# Split year_month into year and month column\n",
    "raw_sunshine_df = raw_sunshine_df.rename(columns={\"month\": \"year_month\"}, errors=\"raise\")\n",
    "year_col = pd.Series([], dtype='int32')\n",
    "month_col = pd.Series([], dtype='int32')\n",
    "for i in range(len(raw_sunshine_df)):\n",
    "    year_col[i] = int(raw_sunshine_df['year_month'][i].split('-')[0])\n",
    "    month_col[i] = int(raw_sunshine_df['year_month'][i].split('-')[1])\n",
    "raw_sunshine_df.insert(1, 'year', year_col)\n",
    "raw_sunshine_df.insert(2, 'month', month_col)\n",
    "sunshine_df = raw_sunshine_df.drop('year_month', 1)\n",
    "# Display\n",
    "sunshine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from CSV\n",
    "raw_temperature_df = pd.read_csv(\n",
    "    \"Datasets/Surface Air Temperature Data/surface-air-temperature-monthly-mean-daily-maximum.csv\")\n",
    "# Split year_month into year and month column\n",
    "raw_temperature_df = raw_temperature_df.rename(columns={\"month\": \"year_month\"}, errors=\"raise\")\n",
    "year_col = pd.Series([], dtype='int32')\n",
    "month_col = pd.Series([], dtype='int32')\n",
    "for i in range(len(raw_temperature_df)):\n",
    "    year_col[i] = int(raw_temperature_df['year_month'][i].split('-')[0])\n",
    "    month_col[i] = int(raw_temperature_df['year_month'][i].split('-')[1])\n",
    "raw_temperature_df.insert(1, 'year', year_col)\n",
    "raw_temperature_df.insert(2, 'month', month_col)\n",
    "surface_air_temperature_df = raw_temperature_df.drop('year_month', 1)\n",
    "# Display\n",
    "surface_air_temperature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shall merge the dataframes into one general dataframe **df** based on *year*, *month* and *week*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dengue_df and population_df by year\n",
    "df = dengue_df.merge(population_df, left_on='year', right_on='year').drop('index', 1)\n",
    "\n",
    "# Create new column for cases_per_capita = number / population\n",
    "# Create new column for month = (datetime(year, 1, 1) + timedelta(days=week*7)).momth\n",
    "cases_per_capita = pd.Series([], dtype='float64')\n",
    "month = pd.Series([], dtype='int32')\n",
    "for i in range(len(df)):\n",
    "    if isnan(df['number'][i]):\n",
    "        cases_per_capita[i] = 0\n",
    "    else:\n",
    "        cases_per_capita[i] = int(df['number'][i]) / int(df['population'][i])\n",
    "    year = int(df['year'][i])\n",
    "    week = int(df['week'][i])\n",
    "    if week > 52:\n",
    "        month[i] = 12\n",
    "    else:\n",
    "        month[i] = (datetime(year, 1, 1) + timedelta(days=7*week)).month\n",
    "df.insert(4, 'cases_per_capita', cases_per_capita)\n",
    "df.insert(1, 'month', month)\n",
    "\n",
    "# Merge sunshine and surface air temperature dataframe\n",
    "df = pd.merge(df, sunshine_df, on=['year', 'month'])\n",
    "df = pd.merge(df, surface_air_temperature_df, on=['year', 'month'])\n",
    "\n",
    "# Display dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we scrape the weather data from 2014 to 2019 for 5 weather stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather station\n",
    "# Scrape from websites\n",
    "base_url = \"http://www.weather.gov.sg/files/dailydata/DAILYDATA_\" #S24_201911.csv\n",
    "\n",
    "# (East) Changi = 24, (West) Tuas South = 115, (North) Khatib = 122, (South) Marina Barrage = 108\n",
    "# (Central) Ang Mo Kio = 109\n",
    "station_dict = {24: 'Changi', 115: 'Tuas South', 122: 'Khatib', 108: 'Marina Barrage', 109: 'Ang Mo Kio'}\n",
    "stations_list = [24, 115, 122, 108, 109]\n",
    "\n",
    "# DataFrame template\n",
    "df_template = pd.DataFrame(columns = ['Station', 'Year', 'Month', 'Day', 'Daily Rainfall Total (mm)',\n",
    "       'Highest 30 Min Rainfall (mm)', 'Highest 60 Min Rainfall (mm)',\n",
    "       'Highest 120 Min Rainfall (mm)', 'Mean Temperature (°C)',\n",
    "       'Maximum Temperature (°C)', 'Minimum Temperature (°C)',\n",
    "       'Mean Wind Speed (km/h)', 'Max Wind Speed (km/h)'])\n",
    "weather_df = df_template.copy()\n",
    "\n",
    "# Scrape stations and combine into a single csv file\n",
    "for station in stations_list:\n",
    "    station_string = \"S\"+str(station) if (station>9) else \"S0\"+str(station)\n",
    "    station_df = df_template.copy()\n",
    "    for year in range(2014,2020):\n",
    "        for month in range(1,13):\n",
    "            month_string = str(month) if (month>9) else \"0\"+str(month)\n",
    "            url = base_url+station_string+\"_\"+str(year)+month_string+\".csv\"\n",
    "            # Get csv files to temp files, and store into DataFrame\n",
    "            try:\n",
    "                r = requests.get(url, allow_redirects=True)\n",
    "                open('temp.csv', 'wb').write(r.content)\n",
    "                station_df = station_df.append(pd.read_csv(\"temp.csv\", encoding = \"ISO-8859-1\"))\n",
    "                weather_df = weather_df.append(pd.read_csv(\"temp.csv\", encoding = \"ISO-8859-1\"))\n",
    "            except:\n",
    "                print(url)\n",
    "                continue\n",
    "    # Store into weather data folder\n",
    "    csv_filename = 'Datasets/Weather Data/' + station_dict[station] + '.csv'\n",
    "    station_df.to_csv(csv_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add the week column to the weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weather station \n",
    "station_dict = {24: 'Changi', 115: 'Tuas South', 122: 'Khatib', 108: 'Marina Barrage', 109: 'Ang Mo Kio'}\n",
    "stations_list = [24, 115, 122, 108, 109]\n",
    "\n",
    "#add week column to each weather stations csv\n",
    "for station in stations_list:\n",
    "    week_col = pd.Series([], dtype='int32')\n",
    "    df = pd.read_csv(\"Datasets/Weather Data/\" + station_dict[station] + \".csv\")\n",
    "    for i in range(len(df)):\n",
    "        week_col[i] = datetime(int(df['Year'][i]), int(df['Month'][i]), int(df['Day'][i])).isocalendar()[1]\n",
    "    df.insert(3, 'Week', week_col)\n",
    "    csv_filename = 'Datasets/Weather Data/' + station_dict[station] + '.csv'\n",
    "    df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert PSI data to contain year, month, week and day columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PSI\n",
    "raw_psi_df = pd.read_csv(\"Datasets/PSI Data/psi_df_2016_2019.csv\")\n",
    "\n",
    "#split to year month and week\n",
    "raw_psi_df = raw_psi_df.rename(columns={\"timestamp\": \"year_month_week_day\"}, errors=\"raise\")\n",
    "year_col = pd.Series([], dtype='int32')\n",
    "month_col = pd.Series([], dtype='int32')\n",
    "day_col = pd.Series([], dtype='int32')\n",
    "week_col = pd.Series([], dtype='int32')\n",
    "for i in range(len(raw_psi_df)):\n",
    "    year_col[i] = int(raw_psi_df['year_month_week_day'][i].split('-')[0])\n",
    "    month_col[i] = int(raw_psi_df['year_month_week_day'][i].split('-')[1])\n",
    "    day_col[i] = int(raw_psi_df['year_month_week_day'][i].split('-')[2][0:2])\n",
    "    week_col[i] = datetime(int(year_col[i]), int(month_col[i]), int(day_col[i])).isocalendar()[1]\n",
    "raw_psi_df.insert(0, 'year', year_col)\n",
    "raw_psi_df.insert(1, 'month', month_col)\n",
    "raw_psi_df.insert(2, 'week', week_col)\n",
    "raw_psi_df.insert(3, 'day', day_col)\n",
    "psi_df = raw_psi_df.drop(['year_month_week_day'], 1)\n",
    "\n",
    "#display\n",
    "psi_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge 2014-2019 location dengue cluster into one csv\n",
    "Add year, month, week and day columns\n",
    "Split locality with '/' and '('\n",
    "save as new csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the location data into one csv\n",
    "raw_location_dengue_2014_df = pd.read_csv(\"Datasets/Dengue Location Cluster Data/2014-Dengue-Cluster.csv\")\n",
    "raw_location_dengue_2015_df = pd.read_csv(\"Datasets/Dengue Location Cluster Data/2015-Dengue-Cluster.csv\")\n",
    "raw_location_dengue_2016_df = pd.read_csv(\"Datasets/Dengue Location Cluster Data/2016-Dengue-Cluster.csv\")\n",
    "raw_location_dengue_2017_df = pd.read_csv(\"Datasets/Dengue Location Cluster Data/2017-Dengue-Cluster.csv\", encoding='unicode_escape')\n",
    "raw_location_dengue_2018_df = pd.read_csv(\"Datasets/Dengue Location Cluster Data/2018-Dengue-Cluster.csv\")\n",
    "raw_location_dengue_2019_df = pd.read_csv(\"Datasets/Dengue Location Cluster Data/2019-Dengue-Cluster.csv\")\n",
    "\n",
    "raw_location_dengue_df = raw_location_dengue_2014_df.append([raw_location_dengue_2015_df, raw_location_dengue_2016_df, raw_location_dengue_2017_df,\n",
    "         raw_location_dengue_2018_df, raw_location_dengue_2019_df])\n",
    "\n",
    "#save the raw csv\n",
    "csv_filename = 'Datasets/Dengue Location Cluster Data/' + 'raw_location_dengue' + '.csv'\n",
    "raw_location_dengue_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "raw_location_dengue_df = pd.read_csv(\"Datasets/Dengue Location Cluster Data/raw_location_dengue.csv\")\n",
    "\n",
    "#split year_month_day to year month and week columns\n",
    "raw_location_dengue_df = raw_location_dengue_df.rename(columns={\"date\": \"year_month_week_day\"}, errors=\"raise\")\n",
    "year_col = pd.Series([], dtype='int32')\n",
    "month_col = pd.Series([], dtype='int32')\n",
    "day_col = pd.Series([], dtype='int32')\n",
    "week_col = pd.Series([], dtype='int32')\n",
    "locality_col = pd.Series([], dtype='str')\n",
    "# shortcut_list = ['rd', 'jln', 'lor', 'dr', 'ave', 'st', 'pk', 'gdns', 'nth', \"s'goon\", 'ter']\n",
    "# shortcut_dict = {'rd': 'Road', 'jln': 'Jalan', 'lor': 'Lorong', 'dr': 'Drive', 'ave': 'Avenue',\n",
    "#                 'st': 'Street', 'pk' : 'Park', 'gdns' : 'Gardens', 'nth' : 'North', \"s'goon\" : 'Serangoon',\n",
    "#                 'ter' : 'Terminal'}\n",
    "\n",
    "for i in range(len(raw_location_dengue_df)):\n",
    "#     address = []\n",
    "    year_col[i] = 2000 + int(str(raw_location_dengue_df['year_month_week_day'][i])[0:2])\n",
    "    month_col[i] = int(str(raw_location_dengue_df['year_month_week_day'][i])[2:4])\n",
    "    day_col[i] = int(str(raw_location_dengue_df['year_month_week_day'][i])[4:])\n",
    "    week_col[i] = datetime(int(year_col[i]), int(month_col[i]), int(day_col[i])).isocalendar()[1]\n",
    "    #split location with '/'\n",
    "    locality_col[i] = raw_location_dengue_df['locality'][i].split('/')[0]\n",
    "    #split location with '('\n",
    "    locality_col[i] = locality_col[i].split('(')[0]\n",
    "    locality_col[i] = \"\".join([x if ord(x) < 128 else ' ' for x in locality_col[i]])\n",
    "#     address = locality_col[i].split(' ')\n",
    "#     for j in range(len(address)):\n",
    "#         for shortcut in shortcut_list:\n",
    "#             if(str.lower(address[j]) == shortcut):\n",
    "#                 address[j] = shortcut_dict[shortcut]\n",
    "#                 break\n",
    "#     print(address)\n",
    "#     locality_col[i] = address[0]\n",
    "#     print(locality_col[i])\n",
    "#     for j in range(len(address)):\n",
    "#         if j != 0 and address[j] != '':\n",
    "#             locality_col[i] += \" \" + address[j]\n",
    "    \n",
    "        \n",
    "raw_location_dengue_df.insert(1, 'year', year_col)\n",
    "raw_location_dengue_df.insert(2, 'month', month_col)\n",
    "raw_location_dengue_df.insert(3, 'week', week_col)\n",
    "raw_location_dengue_df.insert(4, 'day', day_col)\n",
    "raw_location_dengue_df.insert(5, 'address', locality_col)\n",
    "location_dengue_df = raw_location_dengue_df.drop(['year_month_week_day', 'locality'], 1)\n",
    "\n",
    "#save\n",
    "csv_filename = 'Datasets/Dengue Location Cluster Data/' + 'location_dengue' + '.csv'\n",
    "location_dengue_df.to_csv(csv_filename, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the buildings.json to a csv file (buildings.json contain all addresses in Singapore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to json file\n",
    "# df = pd.read_json(\"Datasets/buildings.json\", orient=\"records\")\n",
    "\n",
    "# df.head()\n",
    "\n",
    "fileInput = 'Datasets/buildings.json'\n",
    "fileOutput = 'Datasets/buildings.csv'\n",
    "\n",
    "inputFile = open(fileInput)\n",
    "outputFile = open(fileOutput, 'w')\n",
    "data = json.load(inputFile)\n",
    "inputFile.close()\n",
    "\n",
    "output = csv.writer(outputFile)\n",
    "output.writerow(data[0].keys()) \n",
    "\n",
    "for row in data:\n",
    "    output.writerow(row.values())\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find unique locations in the location_dengue cluster csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv of unique locations in dengue location cluster (2014 to 2019)\n",
    "location_dengue_df = pd.read_csv(\"Datasets/Dengue Location Cluster Data/location_dengue.csv\")\n",
    "location_set = set()\n",
    "for i in range(len(location_dengue_df)):\n",
    "    location_set.add(location_dengue_df['address'][i])\n",
    "\n",
    "fileOutput = 'Datasets/unique_locations.csv'\n",
    "outputFile = open(fileOutput, 'w')\n",
    "output = csv.writer(outputFile)\n",
    "output.writerow(['unique location', 'X', 'Y'])\n",
    "for location in location_set:\n",
    "    output.writerow([location])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add x,y coordinates to the unique locations\n",
    "unique_location_df = pd.read_csv(\"Datasets/unique_locations.csv\")\n",
    "open_buildings_df = pd.read_csv(\"Datasets/buildings.csv\")\n",
    "buildings_df = open_buildings_df.drop(['POSTAL'], 1)\n",
    "for i in range(len(unique_location_df)):\n",
    "    for j in range(len(buildings_df)):\n",
    "        if str.upper(str(unique_location_df['unique location'][i])) in str(buildings_df['ADDRESS'][j]):\n",
    "            unique_location_df['X'][i] = buildings_df['X'][j]\n",
    "            unique_location_df['Y'][i] = buildings_df['Y'][j]\n",
    "            break\n",
    "\n",
    "csv_filename = \"Datasets/unique_locations.csv\"            \n",
    "unique_location_df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add x,y coordinates to the unique locations\n",
    "unique_location_df = pd.read_csv(\"Datasets/unique_locations.csv\")\n",
    "open_buildings_df = pd.read_csv(\"Datasets/buildings.csv\")\n",
    "buildings_df = open_buildings_df.drop(['POSTAL'], 1)\n",
    "count = 0\n",
    "locality_col = pd.Series([], dtype='str')\n",
    "\n",
    "for i in range(len(unique_location_df)):\n",
    "    for j in range(len(buildings_df)):\n",
    "        if fuzz.token_set_ratio(str(unique_location_df['unique location'][i]), str(buildings_df['ADDRESS'][j])) >= 60:\n",
    "            count += 1\n",
    "            unique_location_df['X'][i] = buildings_df['X'][j]\n",
    "            unique_location_df['Y'][i] = buildings_df['Y'][j]\n",
    "            locality_col[i] = buildings_df['ADDRESS'][j]\n",
    "            break\n",
    "\n",
    "print(count)\n",
    "csv_filename = \"Datasets/unique_locations_fuzzywuzzy.csv\"\n",
    "unique_location_df.insert(3, 'address from buildings', locality_col)\n",
    "unique_location_df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the dataframe, not to corrupt the original\n",
    "vis_df = df.copy()\n",
    "x_data = range(0, vis_df.shape[0])\n",
    "\n",
    "# columns to visualize\n",
    "columns = ['cases_per_capita', 'mean_sunshine_hrs', 'temp_mean_daily_max']\n",
    "\n",
    "# plot line graphs\n",
    "for column in columns:\n",
    "    f = plt.figure()\n",
    "    ax = f.add_subplot()\n",
    "    ax.plot(x_data, vis_df[column], label=column)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
